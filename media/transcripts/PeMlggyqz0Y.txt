{0.12} Machine learning teaches a computer how to {2.48} perform a task without explicitly {4.44} programming it. Instead, it feeds data into an algorithm to {6.68} gradually improve outcomes with {9.2} experience, similar to how organic life {13.08} learns. The term was coined in 1959 by {15.679} Arthur Samuel at IBM, who was developing {18.199} artificial intelligence that could play {19.92} checkers. Half a century later, predictive models are embedded in many {21.76} of the products we use every day, which {23.48} perform two fundamental jobs: one is to {25.359} classify data—like, is there another car {29.56} on the road, or does this patient have {31.439} cancer? The other is to make predictions {33.879} about future outcomes—like, will the {35.64} stock go up, or which YouTube video do {37.64} you want to watch next? The first step in {39.32} the process is to acquire and clean up {41.68} data—lots and lots of data. The better {43.8} the data represents the problem, the {45.399} better the results; garbage in, garbage {47.52} out. The data needs to have some kind of {49.36} signal to be valuable to the algorithm {51.239} for making predictions. Data {52.96} scientists perform a job called feature {54.8} engineering to transform raw data into {57.239} features that better represent the {59.0} underlying problem. The next step is to {61.199} separate the data into a training set {63.28} and a testing set. The training data is fed {65.6} into an algorithm to build a model; then {67.64} the testing data is used to validate the {69.88} accuracy or error of the model. The next {72.32} step is to choose an algorithm, which {74.119} might be a simple statistical model like {76.159} linear or logistic regression, or a {78.24} decision tree that assigns different {79.88} weights to features in the data. Or you {81.759} might get fancy with a convolutional {83.96} neural network, which is an algorithm {85.759} that also assigns weights to features {87.759} but also takes the input data and {89.52} creates additional features {90.759} automatically. That's extremely {92.479} useful for data sets that contain things {94.52} like images or natural language, where {96.64} manual feature engineering is virtually {98.56} impossible. Every one of these algorithms {100.399} learns to get better by comparing its {102.399} predictions to an error function. If it's {104.399} a classification problem—like, is this {106.479} animal a cat or a dog—the error function {108.96} might be accuracy. If it's a regression {111.079} problem—like, how much will a loaf of {112.799} bread cost next year—then it might be {114.68} mean absolute error. Python is the {116.96} language of choice among data scientists, {119.039} but R and Julia are also popular options, {121.92} and there are many supporting frameworks {123.439} out there to make the process {124.84} approachable. The end result of the {126.36} machine learning process is a model, {128.399} which is just a file that takes some {130.2} input data in the same shape that it was {132.12} trained on, then spits out a prediction {134.2} that tries to minimize the error that it {136.0} was optimized for. It can then be {137.56} embedded on an actual device or deployed {139.68} to the cloud to build a real-world {141.68} product. This has been machine learning {143.36} in 100 seconds. Like and subscribe if you {145.56} want to see more short videos like this {147.319} and leave a comment if you want to see {148.64} more machine learning content on this {150.239} channel. Thanks for watching, and I will {152.04} see you in the next one.
