Machine learning teaches a computer to perform a task without explicitly programming it. Instead, data is fed into an algorithm to gradually improve outcomes with experience, similar to how organic life learns. The term was coined in 1959 by Arthur Samuel at IBM, who was developing artificial intelligence that could play checkers. Half a century later, predictive models are embedded in many of the products we use every day, which perform two fundamental jobs. One is to classify data, like: "Is there another car on the road?" or "Does this patient have cancer?" The other is to make predictions about future outcomes, like: "Will the stock go up?" or "Which YouTube video do you want to watch?"  The first step in the process is to acquire and clean up dataâ€”lots and lots of data. The better the data represents the problem, the better the results; garbage in, garbage out. The data needs some kind of signal to be valuable to the algorithm for making predictions. Data scientists perform a job called feature engineering to transform raw data into features that better represent the underlying problem. The next step is to separate the data into a training set and a testing set. The training data is fed into an algorithm to build a model. Then, the testing data is used to validate the accuracy or error of the model. The next step is to choose an algorithm, which might be a simple statistical model like linear or logistic regression, or a decision tree that assigns different weights to features in the data. Or you might get fancy with a convolutional neural network, which is an algorithm that also assigns weights to features, but also takes the input data and creates additional features automatically. This is extremely useful for datasets that contain things like images or natural language, where manual feature engineering is virtually impossible. Every one of these algorithms learns to get better by comparing its predictions to an error function. If it's a classification problem, like "Is this animal a cat or a dog?", the error function might be accuracy. If it's a regression problem, like "How much will a loaf of bread cost next year?", then it might be mean absolute error. Python is the language of choice among data scientists, but R and Julia are also popular options, and there are many supporting frameworks out there to make the process approachable. The end result of the machine learning process is a model, which is just a file that takes some input data in the same shape that it was trained on, then spits out a prediction that tries to minimize the error it was optimized for. It can then be embedded on an actual device or deployed to the cloud to build a real-world product. This has been machine learning in 100 seconds. Like and subscribe if you want to see more short videos like this, and leave a comment if you want to see more machine learning content on this channel. Thanks for watching, and I will see you in the next one.
