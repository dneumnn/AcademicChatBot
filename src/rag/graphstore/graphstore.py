import logging
from neo4j import GraphDatabase
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

from ..constants.env import NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD

graphstore = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

def get_full_graph_information():
    with graphstore.session() as session:
        # Get all nodes and relationships
        result = session.run("""
            MATCH (n)
            OPTIONAL MATCH (n)-[r]->(m)
            RETURN collect(distinct {
                node: n,
                type: labels(n)[0],
                properties: properties(n)
            }) as nodes,
            collect(distinct {
                start: startNode(r),
                type: type(r),
                end: endNode(r),
                properties: properties(r)
            }) as relationships
        """)
        result_data = result.single()
        print("Nodes:", result_data["nodes"])
        print("Relationships:", result_data["relationships"])
        return result_data

def question_to_graphdb(question: str, llm: ChatOpenAI | ChatOllama | ChatGoogleGenerativeAI, logger: logging.Logger) -> str:
    try:
        with graphstore.session() as session:
            schema = session.run("""
                CALL db.schema.visualization()
                YIELD nodes, relationships
                RETURN nodes, relationships
            """).single()

            logger.info(f"Schema: {schema}")

            samples = session.run("""
                MATCH (n)
                WITH labels(n) as labels, count(n) as count
                RETURN labels, count
                LIMIT 300
            """).data()

            logger.info(f"Samples: {samples}")

            sample_names = session.run("""
                MATCH (n)
                RETURN n.name
                LIMIT 3000
            """).data()

            logger.info(f"Sample names: {sample_names}")

            prompt = f"""Given the following Neo4j graph structure:
            
            Node types and counts: {samples}
            Schema: {schema}
            Sample entity names: {sample_names}

            Convert this question to a Cypher query that will answer it:
            {question}

            Return only the Cypher query, no explanation. No Markdown. No code blocks.
            The result of the query will be used to answer the question.
            The cypher query should try to obtain insights from the graph.
            """

            cypher_query = llm.invoke(prompt)

            cypher_query_content = cypher_query.content

            logger.info(f"Cypher query: {cypher_query_content}")

            result = session.run(cypher_query_content)
            data = result.data()

            metadata = [data]

            logger.info(f"Result: {data}")

            answer_prompt = f"""
            The following is the result of a Cypher query: {data}
            Answer the question: {question}

            This was the Cypher query used to generate the result: {cypher_query_content}

            Use only the provided information to answer the question. Do not use quotations.
            The answer is the result of a query generated by the same question, so assume the result is relevant.
            If you do not know the answer, mention the missing context.
            Make the sentence feel human. Add pronouns and other natural language elements if needed.

            DO NOT MENTION ANYTHING ABOUT THE QUERY OR THE GRAPH. JUST TRY TO ANSWER THE QUESTION.
            DO NOT MENTION NODES OR DATA. DO NOT MAKE GUESSES. DO NOT MAKE ASSUMPTIONS.
            IF YOU DO NOT KNOW THE ANSWER, JUST ADMIT IT.
            BE CONCISE.
            """

            answer = llm.invoke(answer_prompt)
            answer_text = answer.content

            logger.info(f"Answer to the neo4j question: {answer_text}")

            return (answer_text, metadata)
    except Exception as e:
        logger.error(f"Error: {e}")
        return ("", [])

if __name__ == "__main__":
    #print(get_full_graph_information())
    llm = ChatOllama(model="llama3.2")
    #llm = ChatOpenAI(model="gpt-4o")

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    logger.addHandler(console_handler)

    print(question_to_graphdb("Where is the painting?", llm, logger))
